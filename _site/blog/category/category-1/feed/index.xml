<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:wfw="http://wellformedweb.org/CommentAPI/"
     >
  <channel>
    <title>GIS programmer writing Python in Emacs</title>
    <link>http://mishkovskyi.net/blog</link>
    <description>(loop [code '()] (if (perfect? code) code (recur (conj code bug))))</description>
    <pubDate>Thu, 14 Jul 2011 14:35:39 GMT</pubDate>
    <generator>Blogofile</generator>
    <sy:updatePeriod>hourly</sy:updatePeriod>
    <sy:updateFrequency>1</sy:updateFrequency>
    <item>
      <title>Post 2</title>
      <link>http://mishkovskyi.netblog/2009/07/24/post-2</link>
      <pubDate>Fri, 24 Jul 2009 16:20:00 EEST</pubDate>
      <category><![CDATA[Category 2]]></category>
      <category><![CDATA[Category 1]]></category>
      <guid isPermaLink="true">http://mishkovskyi.netblog/2009/07/24/post-2</guid>
      <description>Post 2</description>
      <content:encoded><![CDATA[<p>This is post #2</p>]]></content:encoded>
    </item>
    <item>
      <title>Post 1</title>
      <link>http://mishkovskyi.netblog/2009/07/23/post-1</link>
      <pubDate>Thu, 23 Jul 2009 15:22:00 EEST</pubDate>
      <category><![CDATA[Category 1]]></category>
      <guid isPermaLink="true">http://mishkovskyi.netblog/2009/07/23/post-1</guid>
      <description>Post 1</description>
      <content:encoded><![CDATA[<p>This is post #1
The gibberish (nonsense text) presented here is generated by a very simple computer program. Given some sample text, say Shakespeare, as input, the computer generates output which is random, but which has the same statistical distribution of characters or combinations of characters. (A character may be a letter, a digit, a space, a punctuation mark, etc.)</p>
<p>In level 1 gibberish, the output has the same distribution of single characters as the input. For example, the probability of seeing a character like "e" or "z" or "." will be approximately the same in the output as in the input. In level 2 gibberish, the output has the same distribution of character pairs as the input. For example, the probability of seeing a pair like "th" or "te" or "t." will be approximately the same in the output as in the input. In general, in level n gibberish, the output has the same distribution of groups of n characters (n-tuples) as the input. (The algorithm is a letter-based Markov text generator. Level n gibberish is a Markov chain of order n-1.)</p>
<p>It is amazing how well this simple algorithm works, even for very low level numbers. For example, at level 2, you can easily recognize different languages. At level 3 you can recognize the styles of different authors.</p>
<p>For even more fun, the gibberish generator can easily blend two different languages or two different authors. If the input is simply the text from author A followed by the text from author B, the output will be a smooth blend of the two.</p>
<p>To generate your own gibberish, go to Gibberish Generator.</p>
<p>To see some samples, go to Gibberish Samples.</p>
<p>A final thought: Is the human brain simply a level 100 gibberish generator?</p>
<p>References: Program named Mark V. Shaney (pun on Markov Chain) by Bruce Ellis, Rob Pike, and Don P. Mitchell, publicized in the June, 1989, Scientific American "Computer Recreations" column titled "A potpourri of programmed prose and prosody" by A. K. Dewdney.</p>]]></content:encoded>
    </item>
  </channel>
</rss>
