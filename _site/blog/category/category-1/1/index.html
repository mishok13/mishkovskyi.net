


<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    
  <title>GIS programmer writing Python in Emacs</title>
<link rel="alternate" type="application/rss+xml" title="RSS 2.0" href="/blog/feed" />
<link rel="alternate" type="application/atom+xml" title="Atom 1.0"
href="/blog/feed/atom" />
<link rel='stylesheet' href='/css/pygments_murphy.css' type='text/css' />

<link rel='stylesheet' href='/blogotest/css/site.css' type='text/css' />


  </head>
  <body>
    <div id="content">
      
  <h1><a href="/">GIS programmer writing Python in Emacs</a></h1>
<hr/>


      <div id="main_block">
        <div id="prose_block">
          
  
<div class="blog_post">
  <a name="post-2"></a>
  <h2 class="blog_post_title"><a href="/2009/07/24/post-2" rel="bookmark" title="Permanent Link to Post 2">Post 2</a></h2>
  <small>July 24, 2009 at 04:20 PM | categories: 

<a href='/blog/category/category-2'>Category 2</a>, <a href='/blog/category/category-1'>Category 1</a>
</small><p/>
  <div class="post_prose">
    
  <p>This is post #2</p>

  </div>
</div>



  <hr class="interblog" />
  
<div class="blog_post">
  <a name="post-1"></a>
  <h2 class="blog_post_title"><a href="/2009/07/23/post-1" rel="bookmark" title="Permanent Link to Post 1">Post 1</a></h2>
  <small>July 23, 2009 at 03:22 PM | categories: 

<a href='/blog/category/category-1'>Category 1</a>
</small><p/>
  <div class="post_prose">
    
  <p>This is post #1
The gibberish (nonsense text) presented here is generated by a very simple computer program. Given some sample text, say Shakespeare, as input, the computer generates output which is random, but which has the same statistical distribution of characters or combinations of characters. (A character may be a letter, a digit, a space, a punctuation mark, etc.)</p>
<p>In level 1 gibberish, the output has the same distribution of single characters as the input. For example, the probability of seeing a character like "e" or "z" or "." will be approximately the same in the output as in the input. In level 2 gibberish, the output has the same distribution of character pairs as the input. For example, the probability of seeing a pair like "th" or "te" or "t." will be approximately the same in the output as in the input. In general, in level n gibberish, the output has the same distribution of groups of n characters (n-tuples) as the input. (The algorithm is a letter-based Markov text generator. Level n gibberish is a Markov chain of order n-1.)</p>
<p>It is amazing how well this simple algorithm works, even for very low level numbers. For example, at level 2, you can easily recognize different languages. At level 3 you can recognize the styles of different authors.</p>
<p>For even more fun, the gibberish generator can easily blend two different languages or two different authors. If the input is simply the text from author A followed by the text from author B, the output will be a smooth blend of the two.</p>
<p>To generate your own gibberish, go to Gibberish Generator.</p>
<p>To see some samples, go to Gibberish Samples.</p>
<p>A final thought: Is the human brain simply a level 100 gibberish generator?</p>
<p>References: Program named Mark V. Shaney (pun on Markov Chain) by Bruce Ellis, Rob Pike, and Don P. Mitchell, publicized in the June, 1989, Scientific American "Computer Recreations" column titled "A potpourri of programmed prose and prosody" by A. K. Dewdney.</p>

  </div>
</div>



  <hr class="interblog" />

        </div><!-- End Prose Block -->
      </div><!-- End Main Block -->
      <div id="footer">
        
  <hr/>
  <p id="credits">
Powered by <a href="http://www.blogofile.com">Blogofile</a>.<br/>
<br/>
RSS feeds for <a href="/blog/feed">Entries</a>
<br>
</p>


      </div> <!-- End Footer -->
    </div> <!-- End Content -->
  </body>
</html>




